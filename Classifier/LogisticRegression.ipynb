{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19394</td>\n",
       "      <td>19619</td>\n",
       "      <td>20024</td>\n",
       "      <td>2500</td>\n",
       "      <td>1815</td>\n",
       "      <td>657</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>542653</td>\n",
       "      <td>483003</td>\n",
       "      <td>473944</td>\n",
       "      <td>55000</td>\n",
       "      <td>40000</td>\n",
       "      <td>38000</td>\n",
       "      <td>20239</td>\n",
       "      <td>13750</td>\n",
       "      <td>13770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>221</td>\n",
       "      <td>-159</td>\n",
       "      <td>567</td>\n",
       "      <td>380</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>581</td>\n",
       "      <td>1687</td>\n",
       "      <td>1542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>140000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12211</td>\n",
       "      <td>11793</td>\n",
       "      <td>3719</td>\n",
       "      <td>3329</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13007</td>\n",
       "      <td>13912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13007</td>\n",
       "      <td>1122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      X1  X2  X3  X4  X5  X6  X7  X8  X9 ...     X15     X16     X17  \\\n",
       "0   1   20000   2   2   1  24   2   2  -1  -1 ...       0       0       0   \n",
       "1   2  120000   2   2   2  26  -1   2   0   0 ...    3272    3455    3261   \n",
       "2   3   90000   2   2   2  34   0   0   0   0 ...   14331   14948   15549   \n",
       "3   4   50000   2   2   1  37   0   0   0   0 ...   28314   28959   29547   \n",
       "4   5   50000   1   2   1  57  -1   0  -1   0 ...   20940   19146   19131   \n",
       "5   6   50000   1   1   2  37   0   0   0   0 ...   19394   19619   20024   \n",
       "6   7  500000   1   1   2  29   0   0   0   0 ...  542653  483003  473944   \n",
       "7   8  100000   2   2   2  23   0  -1  -1   0 ...     221    -159     567   \n",
       "8   9  140000   2   3   1  28   0   0   2   0 ...   12211   11793    3719   \n",
       "9  10   20000   1   3   2  35  -2  -2  -2  -2 ...       0   13007   13912   \n",
       "\n",
       "     X18    X19    X20    X21    X22    X23  Y  \n",
       "0      0    689      0      0      0      0  1  \n",
       "1      0   1000   1000   1000      0   2000  1  \n",
       "2   1518   1500   1000   1000   1000   5000  0  \n",
       "3   2000   2019   1200   1100   1069   1000  0  \n",
       "4   2000  36681  10000   9000    689    679  0  \n",
       "5   2500   1815    657   1000   1000    800  0  \n",
       "6  55000  40000  38000  20239  13750  13770  0  \n",
       "7    380    601      0    581   1687   1542  0  \n",
       "8   3329      0    432   1000   1000   1000  0  \n",
       "9      0      0      0  13007   1122      0  0  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data from excel format\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel(open('default of credit card clients.xls','rb'),sheetname='Data') \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select feature list\n",
    "feature_list = ['X6', \n",
    "'X7', \n",
    "'X8',\n",
    "'X9',\n",
    "'X10' ,\n",
    "'X11' ,\n",
    "'X1'  ,\n",
    "'X18' ,\n",
    "'X19' ,\n",
    "'X21' ,\n",
    "'X20' ,\n",
    "'X22' ,\n",
    "'X23' ,\n",
    "'X2'  ,\n",
    "'X3'  ,\n",
    "'X4'  ,\n",
    "'X12' ,\n",
    "'X13' ,\n",
    "'X14' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using selected feature to get new features \n",
    "X = df[feature_list]\n",
    "y = df.Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round0 : \n",
      "0.809875\n",
      "0.814833333333\n",
      "0.814833333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4701\n",
      "          1       0.71      0.24      0.36      1299\n",
      "\n",
      "avg / total       0.80      0.81      0.78      6000\n",
      "\n",
      "#################################\n",
      "Round1 : \n",
      "0.811333333333\n",
      "0.809166666667\n",
      "0.809166666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4693\n",
      "          1       0.67      0.24      0.35      1307\n",
      "\n",
      "avg / total       0.79      0.81      0.77      6000\n",
      "\n",
      "#################################\n",
      "Round2 : \n",
      "0.811375\n",
      "0.8125\n",
      "0.8125\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4671\n",
      "          1       0.71      0.26      0.38      1329\n",
      "\n",
      "avg / total       0.80      0.81      0.78      6000\n",
      "\n",
      "#################################\n",
      "Round3 : \n",
      "0.809541666667\n",
      "0.815166666667\n",
      "0.815166666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      4690\n",
      "          1       0.74      0.24      0.36      1310\n",
      "\n",
      "avg / total       0.80      0.82      0.78      6000\n",
      "\n",
      "#################################\n",
      "Round4 : \n",
      "0.811625\n",
      "0.812\n",
      "0.812\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4682\n",
      "          1       0.70      0.25      0.37      1318\n",
      "\n",
      "avg / total       0.80      0.81      0.78      6000\n",
      "\n",
      "#################################\n",
      "Round5 : \n",
      "0.808875\n",
      "0.814666666667\n",
      "0.814666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4699\n",
      "          1       0.70      0.25      0.37      1301\n",
      "\n",
      "avg / total       0.80      0.81      0.78      6000\n",
      "\n",
      "#################################\n",
      "Round6 : \n",
      "0.80925\n",
      "0.805833333333\n",
      "0.805833333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.97      0.89      4643\n",
      "          1       0.72      0.23      0.35      1357\n",
      "\n",
      "avg / total       0.79      0.81      0.76      6000\n",
      "\n",
      "#################################\n",
      "Round7 : \n",
      "0.810625\n",
      "0.809\n",
      "0.809\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89      4671\n",
      "          1       0.72      0.23      0.35      1329\n",
      "\n",
      "avg / total       0.79      0.81      0.77      6000\n",
      "\n",
      "#################################\n",
      "Round8 : \n",
      "0.811166666667\n",
      "0.803666666667\n",
      "0.803666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.97      0.88      4649\n",
      "          1       0.70      0.23      0.34      1351\n",
      "\n",
      "avg / total       0.79      0.80      0.76      6000\n",
      "\n",
      "#################################\n",
      "Round9 : \n",
      "0.811791666667\n",
      "0.8035\n",
      "0.8035\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.97      0.88      4649\n",
      "          1       0.69      0.23      0.35      1351\n",
      "\n",
      "avg / total       0.79      0.80      0.76      6000\n",
      "\n",
      "#################################\n",
      "0.810033333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "l = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    min_max_scaler = preprocessing.RobustScaler()\n",
    "    x_train_scaled = min_max_scaler.fit_transform(X_train[X_train.columns[0:19]])\n",
    "    x_test_scaled = min_max_scaler.fit_transform(X_test[X_test.columns[0:19]])\n",
    "    X_train_robust = pd.DataFrame(x_train_scaled)\n",
    "    X_test_robust = pd.DataFrame(x_test_scaled)\n",
    "    \n",
    "    log = LogisticRegression()\n",
    "    log.fit(X_train_robust, y_train)\n",
    "    y_pred_class = log.predict(X_test_robust)\n",
    "\n",
    "    l.append(f1_score(y_test, y_pred_class, average='micro'))\n",
    "    print('Round{} : '.format(i))\n",
    "    print(log.score(X_train_robust,y_train))\n",
    "    print(log.score(X_test_robust,y_test))\n",
    "    print(f1_score(y_test, y_pred_class, average='micro'))\n",
    "    print(metrics.classification_report(y_test, y_pred_class))\n",
    "    print('#################################')\n",
    "print(sum(l)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAIYCAYAAACLy3rqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/z8DcbetNqTSKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110a43ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#select the optimal regularization parameter for logistic regression\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#scalar Before classification, we need to normalize the data first.\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "#cross validation seed\n",
    "n_folds = 10\n",
    "kf = KFold(n=len(X), n_folds=n_folds, shuffle=True)\n",
    "#Decisiontree classifier\n",
    "def test_logistic(train_X, train_y, test_X, test_y, penalty = 'l2',C = 1, debug_flag = False):\n",
    "    log = LogisticRegression(penalty = 'l2',C=1)\n",
    "    log.fit(train_X, train_y)\n",
    "    train_error = log.score(train_X, train_y)\n",
    "    test_error = log.score(test_X, test_y)\n",
    "    if debug_flag:\n",
    "        print('=============')\n",
    "        print('training error:\\t{}'.format(train_error))\n",
    "        print('testing error:\\t{}'.format(test_error))\n",
    "    return train_error, test_error\n",
    "def cv(penalty = 'l2',C=1):\n",
    "    train_error_total = 0\n",
    "    test_error_total = 0\n",
    "    for train, test in kf:\n",
    "        train_X = X_scaled[train]\n",
    "        test_X = X_scaled[test]\n",
    "        train_y = y.iloc[train]\n",
    "        test_y = y.iloc[test]\n",
    "        train_error, test_error = test_logistic(train_X, train_y, test_X, test_y,penalty,C)\n",
    "        train_error_total += train_error\n",
    "        test_error_total += test_error\n",
    "    return train_error_total/n_folds, test_error_total/n_folds\n",
    "def cv_plot(penalty = 'l2'):\n",
    "    cv_res = []\n",
    "    for i in np.arange(0,1.1,0.05):\n",
    "        train_error, test_error = cv('l2',i)\n",
    "        cv_res.append([i, train_error, test_error])\n",
    "    cv_res_arr = np.array(cv_res)\n",
    "    plt.figure(figsize=(16,9)) \n",
    "    plt.title('Errors vs  regularization parameter for logistic classifiers')\n",
    "    plot_train, = plt.plot(cv_res_arr[:,0], cv_res_arr[:,1], label='training')\n",
    "    plot_test, = plt.plot(cv_res_arr[:,0], cv_res_arr[:,2], label='testing')\n",
    "    plt.legend(handles=[plot_train, plot_test])\n",
    "    plt.ylim((min(min(cv_res_arr[:,1]), min(cv_res_arr[:,2])) - 0.05, max(max(cv_res_arr[:,1]), max(cv_res_arr[:,2]))+0.05))\n",
    "cv_plot('l2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
